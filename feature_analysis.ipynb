{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nirmal/miniconda3/envs/circuits/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "# from collect_activations import *\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from load_gemma import load_gemma_autoencoders\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['HF_HOME']=\"/mnt/data2/nirmal/scaling_feature_discovery/scaling_feature_discovery/.cache\"\n",
    "from nnsight import LanguageModel\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from einops import einsum\n",
    "# import pandas as pd\n",
    "# from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get gemmascope-2b-it-resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModel.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"cuda\", torch_dtype=\"float16\")\n",
    "model = LanguageModel(\"google/gemma-2-2b-it\", device_map=device,dispatch=True,torch_dtype=\"float16\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "submodule_dict,model = load_gemma_autoencoders(\n",
    "    model,\n",
    "    ae_layers=[0,1,2,3,4,5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    average_l0s={0: 43,1:54,2: 77,3: 42,4: 46,5: 53, 6:56, 7: 57, 8: 59, 9: 61, 10: 66, 11: 70, 12: 72, 13: 75, 14: 73, 15: 68,16:69,17:68},\n",
    "    size=\"65k\",\n",
    "    type=\"res\",\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store activations of each feature on template dataset till steer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=load_dataset(\"nirmalendu01/template_jailbreak\",split=\"train\")\n",
    "template='''<bos><start_of_turn>user\n",
    "{prompt}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "'''\n",
    "cache=defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching latents:   0%|          | 0/1218 [00:00<?, ?it/s]You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Caching latents:  15%|█▌        | 186/1218 [01:09<09:10,  1.88it/s]"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(dataset), desc=\"Caching latents\") as pbar:\n",
    "    for i,item in enumerate(iter(dataset)):\n",
    "        buffer = {}\n",
    "        with torch.no_grad():\n",
    "            with model.trace(template.format(prompt=item[\"test_case\"])):\n",
    "                for module_path, submodule in submodule_dict.items():\n",
    "                    buffer[module_path] = submodule.ae.output.save()\n",
    "            for module_path, latents in buffer.items():\n",
    "                cache[module_path].extend(list(zip([i]*len(latents),latents.tolist())))\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cache[module_path][3][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sexual/Adult content 84\n",
      "Economic harm 126\n",
      "Expert advice 126\n",
      "Physical harm 126\n",
      "Malware/Hacking 126\n",
      "Privacy 126\n",
      "Fraud/Deception 126\n",
      "Government decision-making 126\n",
      "Disinformation 126\n",
      "Harassment/Discrimination 126\n"
     ]
    }
   ],
   "source": [
    "sample_categories=defaultdict(list)\n",
    "for i,item in enumerate(iter(dataset)):\n",
    "    sample_categories[item[\"category\"]].append(i)\n",
    "for k,v in sample_categories.items():\n",
    "    print(k,len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fetch gradients and filter features which align with gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65536, 2304])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.mul(F.normalize(model.model.layers[layer].ae.ae.W_dec,p=2,dim=1),gradients[layer].squeeze(0).T).shape\n",
    "# einsum(F.normalize(model.model.layers[layer].ae.ae.W_dec,p=2,dim=1),gradients[layer].squeeze(0),\"ij,kj->ik\").shape\n",
    "# torch.matmul(F.normalize(model.model.layers[layer].ae.ae.W_dec,p=2,dim=1),gradients[layer].squeeze(0).T).shape\n",
    "model.model.layers[0].ae.ae.W_dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "1218it [07:55,  2.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# del model\n",
    "direction = torch.load(\"./load_gemma/direction.pt\").to(device)\n",
    "direction = direction / torch.norm(direction)\n",
    "\n",
    "# model = LanguageModel(\"google/gemma-2-2b-it\", device_map=device)\n",
    "\n",
    "\n",
    "feature_dict=defaultdict(list)\n",
    "threshold=0.8\n",
    "downstream_layer=15\n",
    "for i,item in tqdm(enumerate(iter(dataset))):\n",
    "    gradients=get_gradients(model, template.format(prompt=item[\"test_case\"]), direction)\n",
    "    for layer in range(downstream_layer):\n",
    "        cosine_sim=torch.matmul(F.normalize(model.model.layers[layer].ae.ae.W_dec,p=2,dim=1),gradients[layer].squeeze(0).T)\n",
    "        indices = torch.nonzero(cosine_sim > threshold, as_tuple=True)[0]\n",
    "        feature_dict[layer].extend(indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(feature_dict[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(feature_dict, \"feature_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform t-test on the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module_path, latents in cache.items():\n",
    "    for feature_id in range(65536):\n",
    "        category_activations=[]\n",
    "        non_category_activations=[]\n",
    "        for category, samples in sample_categories.items():\n",
    "            acts1 = [item[feature_id] for i,latent in enumerate(latents) if i in samples for item in latent[1]]\n",
    "            category_activations.extend(latents)\n",
    "            acts2 = [item[feature_id] for i,latent in enumerate(latents) if i not in samples for item in latent[1]]\n",
    "            non_category_activations.extend(latents)\n",
    "            t_stat, p_value = stats.ttest_ind(acts1, acts2)\n",
    "            if np.isnan(p_value) or p_value > 0.05:\n",
    "                continue\n",
    "            print(module_path, feature_id, category, t_stat, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare sample identified feature of each type to neuronpedia explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ablate features and measure impact on template dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
