{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from collect_activations import *\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from load_gemma import load_gemma_autoencoders\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "os.environ['HF_HOME']=\"/mnt/data2/nirmal/scaling_feature_discovery/scaling_feature_discovery/.cache\"\n",
    "from nnsight import LanguageModel\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get gemmascope-2b-it-resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8803a077efef4977819cb000c834e280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ba08df7c1b4a4c8af732912a429dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = AutoModel.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"cuda\", torch_dtype=\"float16\")\n",
    "model = LanguageModel(\"google/gemma-2-2b-it\", device_map=\"cuda\",dispatch=True,torch_dtype=\"float16\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "submodule_dict,model = load_gemma_autoencoders(\n",
    "    model,\n",
    "    ae_layers=[0,1,2,3,4,5],\n",
    "    average_l0s={0: 43,1:54,2: 77,3: 42,4: 46,5: 53},\n",
    "    size=\"65k\",\n",
    "    type=\"res\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store activations of each feature on template dataset till steer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdb708b847f4efb8b2cecf8f901f041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec51cfb151da402ea01b0c61df080d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df695e788a6c415596b647b850bce72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1218 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset=load_dataset(\"nirmalendu01/template_jailbreak\",split=\"train\")\n",
    "template='''<bos><start_of_turn>user\n",
    "{prompt}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "'''\n",
    "cache=defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching latents:   0%|          | 0/1218 [00:00<?, ?it/s]You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n",
      "Caching latents: 100%|██████████| 1218/1218 [05:46<00:00,  3.52it/s]  \n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(dataset), desc=\"Caching latents\") as pbar:\n",
    "    for i,item in enumerate(iter(dataset)):\n",
    "        buffer = {}\n",
    "        with torch.no_grad():\n",
    "            with model.trace(template.format(prompt=item[\"test_case\"])):\n",
    "                for module_path, submodule in submodule_dict.items():\n",
    "                    buffer[module_path] = submodule.ae.output.save()\n",
    "            for module_path, latents in buffer.items():\n",
    "                cache[module_path].extend(list(zip([i]*len(latents),latents.tolist())))\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cache[module_path][3][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sexual/Adult content 84\n",
      "Economic harm 126\n",
      "Expert advice 126\n",
      "Physical harm 126\n",
      "Malware/Hacking 126\n",
      "Privacy 126\n",
      "Fraud/Deception 126\n",
      "Government decision-making 126\n",
      "Disinformation 126\n",
      "Harassment/Discrimination 126\n"
     ]
    }
   ],
   "source": [
    "sample_categories=defaultdict(list)\n",
    "for i,item in enumerate(iter(dataset)):\n",
    "    sample_categories[item[\"category\"]].append(i)\n",
    "for k,v in sample_categories.items():\n",
    "    print(k,len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fetch gradients and filter features which align with gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = torch.load(\"./load_gemma/direction.pt\").cuda()\n",
    "direction = direction / torch.norm(direction)\n",
    "feature_dict=defaultdict(list)\n",
    "threshold=0.3\n",
    "downstream_layer=10\n",
    "for i,item in enumerate(iter(dataset)):\n",
    "    gradients=get_gradients(model, item[\"test_case\"], direction)\n",
    "    for layer in range(downstream_layer):\n",
    "        cosine_sim=torch.mul(model.model.layers[layer].sae.W_dec.norm(dim=1),gradients[layer])\n",
    "        indices = torch.nonzero(cosine_sim > threshold, as_tuple=True)[0]\n",
    "        feature_dict[layer].extend(indices.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity with gradients with threshold to filter feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform t-test on the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module_path, latents in cache.items():\n",
    "    for feature_id in range(65536):\n",
    "        category_activations=[]\n",
    "        non_category_activations=[]\n",
    "        for category, samples in sample_categories.items():\n",
    "            acts1 = [item[feature_id] for i,latent in enumerate(latents) if i in samples for item in latent[1]]\n",
    "            category_activations.extend(latents)\n",
    "            acts2 = [item[feature_id] for i,latent in enumerate(latents) if i not in samples for item in latent[1]]\n",
    "            non_category_activations.extend(latents)\n",
    "            t_stat, p_value = stats.ttest_ind(acts1, acts2)\n",
    "            if np.isnan(p_value) or p_value > 0.05:\n",
    "                continue\n",
    "            print(module_path, feature_id, category, t_stat, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare sample identified feature of each type to neuronpedia explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ablate features and measure impact on template dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
