{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a3561d0",
   "metadata": {},
   "source": [
    "# Transfer between chat and base SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71678762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-25 00:09:18 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x15555062cb50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.utils import *\n",
    "from utils.plot_utils import *\n",
    "from utils.data_utils import *\n",
    "from utils.eval_refusal import *\n",
    "from utils.attribution_utils import *\n",
    "from utils.model_utils import *\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from utils.gemmascope import JumpReLUSAE\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.set_grad_enabled(False) # rmb set to true for grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37da668d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3433a6b7f954c5ba389ac8102b31d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Load model and SAE\n",
    "\n",
    "device = 'cuda:0'\n",
    "torch_dtype = torch.bfloat16\n",
    "model_name = \"gemma-2b\"\n",
    "# model_name = 'llama'\n",
    "model = load_tl_model(model_name,device = device, torch_dtype = torch_dtype)\n",
    "num_sae_layer = model.cfg.n_layers\n",
    "saes = load_sae(model_name,num_sae_layer,device=device, torch_dtype=torch_dtype,split_device = False)\n",
    "size = model_sizes[model_name]\n",
    "model.model_name = model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdaa793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "torch_dtype = torch.bfloat16\n",
    "chat_saes = {}\n",
    "chat_device = 'cuda:0'\n",
    "layer_to_eval = [12,13,14,15,16]\n",
    "for layer_ in layer_to_eval:\n",
    "    if layer_ in [15,13]:\n",
    "        chat_path = f\"checkpoints/gemma2-2b-chat-1m-15-13/best/layers.{layer_}/layers.{layer_}/sae.safetensors\"\n",
    "    elif layer_ in [14,16]:\n",
    "        chat_path = f\"checkpoints/gemma2-2b-chat-1m-14-16/best/layers.{layer_}/layers.{layer_}/sae.safetensors\"\n",
    "    else:\n",
    "        chat_path = \"checkpoints/gemma2-2b-chat-1m-12/best/layers.12/layers.12/sae.safetensors\"\n",
    "    chat_saes[sae_naming['res'].format(l=layer_)] = JumpReLUSAE.from_pretrained(\n",
    "        chat_path, device = device,is_hf=False\n",
    "    ).to(torch_dtype).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cef6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_test_size = 1000 # take 1000 samples\n",
    "pile_bz = 20\n",
    "num_pile_iter = ce_test_size//pile_bz\n",
    "pile_iterator = load_pile_iterator(pile_bz,model.tokenizer,device=model.cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34016d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_ce_loss(inps,loss_mask,model,avg=True): # only input\n",
    "    logits = model(inps['input_ids'],attention_mask = inps['attention_mask'])\n",
    "    logprobs = F.log_softmax(logits, dim=-1)\n",
    "    log_probs_for_labels = logprobs[:, :-1].gather(dim=-1, index=inps['input_ids'][:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    log_probs_for_labels = torch.cat(\n",
    "            [\n",
    "                log_probs_for_labels,\n",
    "                torch.zeros(log_probs_for_labels.shape[0]).unsqueeze(-1).to(log_probs_for_labels)\n",
    "            ],\n",
    "            dim=-1\n",
    "        )\n",
    "    ce_loss = -(log_probs_for_labels * loss_mask).mean(dim=-1)\n",
    "    if avg:\n",
    "        return ce_loss.mean().item()\n",
    "    else:\n",
    "        return ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec4cc3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacement_hook(act,hook,saes,pos_mask=None): # pos mask is the mask to replace the activations with reconstructed ones\n",
    "    reconstr = saes[hook.name].decode(saes[hook.name].encode(act.to(saes[hook.name].W_dec.device)))\n",
    "    if pos_mask is None:\n",
    "        act = reconstr.to(act.device)\n",
    "    else:\n",
    "        act[pos_mask,:] = reconstr[pos_mask.to(reconstr.device),:].to(act.device)\n",
    "    return act\n",
    "\n",
    "def zero_hook(act,hook):\n",
    "    act = act * 0\n",
    "    return act"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a5ab03",
   "metadata": {},
   "source": [
    "# Eval on w/o assistant tokens on The Pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f030d442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean loss: 2.8597\n",
      "Layer: 12, CE/Recovered loss: base: 3.1391/0.9648, chat: 3.0049/0.9834\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer: 13, CE/Recovered loss: base: 3.1731/0.9633, chat: 3.0748/0.9765\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer: 14, CE/Recovered loss: base: 3.1502/0.9676, chat: 3.1881/0.9626\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer: 15, CE/Recovered loss: base: 2.9873/0.9867, chat: 2.9338/0.9931\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer: 16, CE/Recovered loss: base: 3.0534/0.9788, chat: 2.9838/0.9864\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "base_losses = []\n",
    "zero_losses = defaultdict(list)\n",
    "base_sae_losses = defaultdict(list)\n",
    "chat_sae_losses = defaultdict(list)\n",
    "\n",
    "batch_no = 0\n",
    "model.reset_hooks()\n",
    "for pile_inputs,loss_mask in pile_iterator:\n",
    "    if batch_no >= num_pile_iter:\n",
    "        break\n",
    "    bos_pos = (pile_inputs['input_ids'] == model.tokenizer.bos_token_id).int().argmax(dim=1)\n",
    "    # create a position mask for input > the bos_pos\n",
    "    pos_mask = torch.arange(pile_inputs['input_ids'].shape[1], device=pile_inputs['input_ids'].device).unsqueeze(0) > bos_pos.unsqueeze(1)\n",
    "    # base loss\n",
    "    base_loss = get_input_ce_loss(pile_inputs,loss_mask,model,avg=False)\n",
    "    base_losses.extend(base_loss.tolist())\n",
    "\n",
    "\n",
    "    for curr_layer in layer_to_eval:\n",
    "        layer_to_hook = f'blocks.{curr_layer}.hook_resid_post'\n",
    "        ## zero loss\n",
    "        model.reset_hooks()\n",
    "        model.add_hook(layer_to_hook,zero_hook)\n",
    "        zero_loss = get_input_ce_loss(pile_inputs,loss_mask,model,avg=False)\n",
    "        zero_losses[curr_layer].extend(zero_loss.tolist())\n",
    "        model.reset_hooks()\n",
    "\n",
    "        ## replace loss\n",
    "        for j,sae_ in enumerate([saes,chat_saes]):\n",
    "            model.reset_hooks()\n",
    "            model.add_hook(layer_to_hook,partial(replacement_hook,saes=sae_,pos_mask =  pos_mask))\n",
    "            sae_loss = get_input_ce_loss(pile_inputs,loss_mask,model,avg=False)\n",
    "            model.reset_hooks()\n",
    "\n",
    "            if j == 0:\n",
    "                base_sae_losses[curr_layer].extend(sae_loss.tolist())\n",
    "            else:\n",
    "                chat_sae_losses[curr_layer].extend(sae_loss.tolist())\n",
    "    batch_no += 1\n",
    "\n",
    "print (f'Clean loss: {np.mean(base_losses):.4f}')\n",
    "\n",
    "base_loss_tensor = torch.tensor(base_losses)\n",
    "for layer in base_sae_losses.keys():\n",
    "    zero_loss = torch.tensor(zero_losses[layer])\n",
    "    base_sae_loss = torch.tensor(base_sae_losses[layer])\n",
    "    chat_sae_loss = torch.tensor(chat_sae_losses[layer])\n",
    "    div_val = zero_loss - base_loss_tensor\n",
    "    div_val[torch.abs(div_val) < 0.0001] = 1.0\n",
    "\n",
    "    base_rec_loss = ((zero_loss - base_sae_loss)/ div_val).mean().item()\n",
    "    chat_rec_loss = ((zero_loss - chat_sae_loss)/ div_val).mean().item()\n",
    "\n",
    "    print (f'Layer: {layer}, CE/Recovered loss: base: {base_sae_loss.mean().item():.4f}/{base_rec_loss:.4f}, chat: {chat_sae_loss.mean().item():.4f}/{chat_rec_loss:.4f}')\n",
    "    print (f'--'*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0badf004",
   "metadata": {},
   "source": [
    "# Evaluate rollouts from alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51d79213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:24, 16.90s/it]                                                                                                                                                                                                                                                                             \n"
     ]
    }
   ],
   "source": [
    "ce_test_size = 300\n",
    "_, _, _, alpaca_ds = load_refusal_datasets(val_size=ce_test_size)\n",
    "bz = 64\n",
    "base_alpaca_outputs = batch_generate(alpaca_ds,model,bz = bz,saes=saes,steering_fn = None,max_new_tokens=256,use_tqdm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e54d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ce_loss(inps,labels,bz,model,use_tqdm = False,use_avg = True): # get ce loss on labels\n",
    "    all_ce_loss = []\n",
    "    to_iter = tqdm(range(0,len(inps),bz),total = len(inps)//bz) if use_tqdm else range(0,len(inps),bz)\n",
    "    for i in to_iter:\n",
    "        formatted_inp = [format_prompt(model.tokenizer,x) for x in inps[i:i+bz]]\n",
    "        batch_labels = labels[i:i+bz]\n",
    "        combined = [inp_ + lab_ for inp_,lab_ in zip(formatted_inp,batch_labels)]\n",
    "        encoded_inp = encode_fn(combined,model)\n",
    "        encoded_label_len = [len(model.tokenizer.encode(lab)) for lab in batch_labels]\n",
    "        loss_mask = []\n",
    "        for i,mask in enumerate(encoded_inp['attention_mask']):\n",
    "            temp_loss_mask = mask.clone()\n",
    "            temp_loss_mask[:-(encoded_label_len[i]+1)] = 0 # mask off input\n",
    "            loss_mask.append(temp_loss_mask)\n",
    "        loss_mask = torch.stack(loss_mask).to(encoded_inp['input_ids'])\n",
    "        if use_avg:\n",
    "            all_ce_loss.append(get_input_ce_loss(encoded_inp,loss_mask,model))\n",
    "        else:\n",
    "            all_ce_loss.extend(get_input_ce_loss(encoded_inp,loss_mask,model,avg=False).tolist())\n",
    "            \n",
    "    return np.mean(all_ce_loss) if use_avg else np.array(all_ce_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58d5ac39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Alpaca Loss: 0.2989\n",
      "12 base: ce loss: 0.4239, recovered: 0.9857\n",
      "12 chat: ce loss: 0.3307, recovered: 0.9961\n",
      "13 base: ce loss: 0.4637, recovered: 0.9808\n",
      "13 chat: ce loss: 0.3350, recovered: 0.9956\n",
      "14 base: ce loss: 0.4781, recovered: 0.9789\n",
      "14 chat: ce loss: 0.3277, recovered: 0.9964\n",
      "15 base: ce loss: 0.4452, recovered: 0.9831\n",
      "15 chat: ce loss: 0.3207, recovered: 0.9975\n",
      "16 base: ce loss: 0.4270, recovered: 0.9854\n",
      "16 chat: ce loss: 0.3274, recovered: 0.9967\n"
     ]
    }
   ],
   "source": [
    "base_alpaca_loss = get_ce_loss(alpaca_ds,base_alpaca_outputs,bz,model,use_avg=False)\n",
    "print (f'Base Alpaca Loss: {base_alpaca_loss.mean().item():.4f}')\n",
    "\n",
    "for curr_layer in layer_to_eval:\n",
    "    layer_to_hook = f'blocks.{curr_layer}.hook_resid_post'\n",
    "    model.reset_hooks()\n",
    "    model.add_hook(layer_to_hook,zero_hook)\n",
    "    zero_loss = get_ce_loss(alpaca_ds,base_alpaca_outputs,bz,model,use_avg=False)\n",
    "    model.reset_hooks()\n",
    "\n",
    "    saes_loss = []\n",
    "    for j,sae_ in enumerate([saes,chat_saes]):\n",
    "        model.reset_hooks()\n",
    "        per_sae_loss = []\n",
    "        ## manually do the ce loss here\n",
    "        for i in range(0,len(alpaca_ds),bz):\n",
    "            formatted_inp = [format_prompt(model.tokenizer,x) for x in alpaca_ds[i:i+bz]]\n",
    "            batch_labels = base_alpaca_outputs[i:i+bz]\n",
    "            combined = [inp_ + lab_ for inp_,lab_ in zip(formatted_inp,batch_labels)]\n",
    "            encoded_inp = encode_fn(combined,model)\n",
    "            encoded_label_len = [len(model.tokenizer.encode(lab)) for lab in batch_labels]\n",
    "            loss_mask = []\n",
    "            for i,mask in enumerate(encoded_inp['attention_mask']):\n",
    "                temp_loss_mask = mask.clone()\n",
    "                temp_loss_mask[:-(encoded_label_len[i]+1)] = 0 # mask off input\n",
    "                loss_mask.append(temp_loss_mask)\n",
    "            loss_mask = torch.stack(loss_mask).to(encoded_inp['input_ids'])\n",
    "            pos_mask = loss_mask.clone().bool()\n",
    "\n",
    "            model.add_hook(layer_to_hook,partial(replacement_hook,saes=sae_,pos_mask =  pos_mask))\n",
    "            sae_loss = get_input_ce_loss(encoded_inp,loss_mask,model,avg = False).tolist()\n",
    "            model.reset_hooks()\n",
    "            per_sae_loss.extend(sae_loss)\n",
    "        saes_loss.append(np.array(per_sae_loss))\n",
    "    \n",
    "    div_val = zero_loss - base_alpaca_loss\n",
    "    div_val[np.abs(div_val) < 0.0001] = 1.0\n",
    "    for j,s_l in enumerate(saes_loss):\n",
    "        rec_loss = ((zero_loss - s_l)/ div_val).mean().item()\n",
    "        saes_name = 'base' if j == 0 else 'chat'\n",
    "        print (f'{curr_layer} {saes_name}: ce loss: {s_l.mean().item():.4f}, recovered: {rec_loss:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf2bda",
   "metadata": {},
   "source": [
    "# Reconstruct the special chat tokens on harmful instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a3b6156",
   "metadata": {},
   "outputs": [],
   "source": [
    "harm_ds_names = ['harmbench_test','jailbreakbench','advbench']\n",
    "harm_ds = {name:load_all_dataset(name,instructions_only=True)[:100] for name in harm_ds_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe963273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 12\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "clean: 5.6458\n",
      "base: 5.6354\n",
      "chat: 5.7917\n",
      "base_recover: 0.9648\n",
      "chat_recover: 0.9258\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Layer: 13\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "clean: 5.6458\n",
      "base: 5.5625\n",
      "chat: 5.7188\n",
      "base_recover: 1.0091\n",
      "chat_recover: 0.9844\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Layer: 14\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "clean: 5.6458\n",
      "base: 5.5000\n",
      "chat: 5.6771\n",
      "base_recover: 1.0573\n",
      "chat_recover: 1.0052\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Layer: 15\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "clean: 5.6458\n",
      "base: 5.3646\n",
      "chat: 5.6250\n",
      "base_recover: 1.1042\n",
      "chat_recover: 1.0091\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Layer: 16\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "clean: 5.6458\n",
      "base: 5.5833\n",
      "chat: 5.7083\n",
      "base_recover: 1.0273\n",
      "chat_recover: 0.9766\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for curr_layer in layer_to_eval:\n",
    "    layer_to_hook = f'blocks.{curr_layer}.hook_resid_post'\n",
    "    all_ce_loss = defaultdict(list)\n",
    "    model.reset_hooks()\n",
    "    for name,ds in harm_ds.items():\n",
    "        harmful_inps = encode_fn([format_prompt(model.tokenizer,x) for x in ds],model)\n",
    "        loss_mask = harmful_inps['attention_mask'].clone()\n",
    "\n",
    "        bos_pos = (harmful_inps['input_ids'] == model.tokenizer.bos_token_id).int().argmax(dim=1)\n",
    "        pos_mask = torch.arange(harmful_inps['input_ids'].shape[1], device=harmful_inps['input_ids'].device).unsqueeze(0) > bos_pos.unsqueeze(1)\n",
    "\n",
    "        # base loss\n",
    "        base_loss = get_input_ce_loss(harmful_inps,loss_mask,model,avg=False)\n",
    "        all_ce_loss['clean'].append(base_loss.mean().item())\n",
    "\n",
    "        ## zero loss\n",
    "        model.reset_hooks()\n",
    "        model.add_hook(layer_to_hook,zero_hook)\n",
    "        zero_loss = get_input_ce_loss(harmful_inps,loss_mask,model,avg=False)\n",
    "        model.reset_hooks()\n",
    "\n",
    "        saes_loss = []\n",
    "        for j,sae_ in enumerate([saes,chat_saes]):\n",
    "            model.reset_hooks()\n",
    "            model.add_hook(layer_to_hook,partial(replacement_hook,saes=sae_,pos_mask =  pos_mask))\n",
    "            sae_loss = get_input_ce_loss(harmful_inps,loss_mask,model,avg=False)\n",
    "            model.reset_hooks()\n",
    "            saes_loss.append(sae_loss)\n",
    "            sae_name = 'base' if j == 0 else 'chat'\n",
    "            all_ce_loss[sae_name].append(sae_loss.mean().item())\n",
    "        \n",
    "        div_val = zero_loss - base_loss\n",
    "        # div_val[torch.abs(div_val) < 0.0001] = 1.0\n",
    "        for j,s_l in enumerate(saes_loss):\n",
    "            rec_loss = ((zero_loss - s_l)/ div_val).mean().item()\n",
    "            sae_name = 'base_recover' if j == 0 else 'chat_recover'\n",
    "            all_ce_loss[sae_name].append(rec_loss)\n",
    "            \n",
    "    print (f'Layer: {curr_layer}')\n",
    "    print (f'--'*60)\n",
    "    print (f'')\n",
    "    for name,loss in all_ce_loss.items():\n",
    "        print(f'{name}: {np.mean(loss):.4f}')\n",
    "    print (f'--'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3ba3d5",
   "metadata": {},
   "source": [
    "# Reconstruct the refusal direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "90d3fabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base cosine similarity: 0.8607\n",
      "chat cosine similarity: 0.9883\n"
     ]
    }
   ],
   "source": [
    "harmful_train, harmless_train, _, harmless_val = load_refusal_datasets() \n",
    "is_base_harmless,base_harmless_logit = batch_single(harmless_train,model,eval_refusal=True,avg_samples=False)\n",
    "harmless_train = [x for x,y in zip(harmless_train,is_base_harmless) if not y]\n",
    "\n",
    "harmless_inps  = encode_fn([format_prompt(model.tokenizer,x) for x in harmless_train[:100]],model)\n",
    "layer_to_retrieve = f'blocks.15.hook_resid_post'\n",
    "\n",
    "steer_vecs = defaultdict(list)\n",
    "for ds in harm_ds.values():\n",
    "    harmful_inps  = encode_fn([format_prompt(model.tokenizer,x) for x in ds],model)\n",
    "    for reconstruct_name in ['clean','base','chat']:\n",
    "        model.reset_hooks()\n",
    "        if reconstruct_name in ['base','chat']:\n",
    "            model.add_hook(layer_to_retrieve,partial(replacement_hook,saes=saes if reconstruct_name == 'base' else chat_saes))\n",
    "        _,harmful_cache = model.run_with_cache(harmful_inps.input_ids,attention_mask = harmful_inps.attention_mask,names_filter = layer_to_retrieve)\n",
    "        _,harmless_cache = model.run_with_cache(harmless_inps.input_ids,attention_mask = harmless_inps.attention_mask,names_filter = layer_to_retrieve)\n",
    "\n",
    "        steer_vec = harmful_cache[layer_to_retrieve][:,-1].mean(0) - harmless_cache[layer_to_retrieve][:,-1].mean(0)\n",
    "        del harmful_cache, harmless_cache\n",
    "        steer_vecs[reconstruct_name].append(steer_vec)\n",
    "    model.reset_hooks()\n",
    "\n",
    "## print cosine similarity\n",
    "reconst_cosine_sim = defaultdict(list)\n",
    "for name in ['base','chat']:\n",
    "    for i,clean_vec in enumerate(steer_vecs['clean']):\n",
    "        cosine_sim = F.cosine_similarity(clean_vec,steer_vecs[name][i],dim=0)\n",
    "        reconst_cosine_sim[name].append(cosine_sim.item())\n",
    "\n",
    "for name,cosine_sim in reconst_cosine_sim.items():\n",
    "    print(f'{name} cosine similarity: {np.mean(cosine_sim):.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
